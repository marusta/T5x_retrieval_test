{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0BQWhvAP2jb"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqZYp90PIa1t"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This is the third Colab in a [series of tutorials on how to use T5X](https://github.com/google-research/t5x/blob/main/docs/tutorials.md). We assume that you have already completed the [Introductory Colab](https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/introduction.ipynb) and the [Training Deep Dive](https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/training.ipynb), or have a basic understanding of the T5X models, checkpoints, partitioner, trainer, and `InteractiveModel`.\n",
        "\n",
        "In the [previous Colab](https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/training.ipynb) in this tutorial series, we dove into how the InteractiveModel restores models from checkpoints and runs training, while also getting an introduction to the T5X trainer. In this Colab, we will focus on how the `InteractiveModel` does decoding to generate predictions and scores for a given input. It should be noted that the code snippets below exactly replicate the InteractiveModel `__init__()` and `infer_with_preprocessors()` methods (see [source code](https://github.com/google-research/t5x/blob/main/t5x/interactive_model.py)); we expose this functionality here in order to demonstrate how various components of the T5X codebase work together to run inference on a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZJbWZcfkyxI"
      },
      "source": [
        "# Set-Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8my9yhSRi6GG"
      },
      "source": [
        "Note: If you are a using public colab, please use its `Connect to a local runtime` option by following the [setup guide](https://github.com/google-research/t5x/blob/main/t5x/notebooks/README.md)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install t5"
      ],
      "metadata": {
        "id": "C4NtcA5DdYeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIGSIHzD7YPO"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Sequence\n",
        "import enum\n",
        "import functools\n",
        "import inspect\n",
        "import itertools\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from typing import Any, Callable, Iterator, Optional, Tuple, Union\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from jax.experimental import multihost_utils\n",
        "import numpy as np\n",
        "import seqio\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import t5.data\n",
        "import t5.data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/airio.git"
      ],
      "metadata": {
        "id": "S8upEaftgQWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Assuming you cloned it into /content/airio\n",
        "REPO_PATH = 'airio'\n",
        "\n",
        "# Add the main repository directory\n",
        "if REPO_PATH not in sys.path:\n",
        "    sys.path.append(REPO_PATH)\n",
        "\n",
        "# If the actual Python source code (the 'airio' package itself) is in a subfolder,\n",
        "# you may need to add that too. Check the structure of the cloned repo.\n",
        "# Example: If the source is in /content/airio/src:\n",
        "# SRC_PATH = os.path.join(REPO_PATH, 'src')\n",
        "# if SRC_PATH not in sys.path:\n",
        "#     sys.path.append(SRC_PATH)\n",
        "\n",
        "print(f\"Added {REPO_PATH} to sys.path.\")"
      ],
      "metadata": {
        "id": "tDKwzBlVg_tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Assuming you cloned it into /content/airio\n",
        "REPO_PATH = 't5x'\n",
        "\n",
        "# Add the main repository directory\n",
        "if REPO_PATH not in sys.path:\n",
        "    sys.path.append(REPO_PATH)\n",
        "\n",
        "# If the actual Python source code (the 'airio' package itself) is in a subfolder,\n",
        "# you may need to add that too. Check the structure of the cloned repo.\n",
        "# Example: If the source is in /content/airio/src:\n",
        "# SRC_PATH = os.path.join(REPO_PATH, 'src')\n",
        "# if SRC_PATH not in sys.path:\n",
        "#     sys.path.append(SRC_PATH)\n",
        "\n",
        "print(f\"Added {REPO_PATH} to sys.path.\")"
      ],
      "metadata": {
        "id": "kjDAz2WFiQBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNtayQIxjEBd"
      },
      "outputs": [],
      "source": [
        "import clu.data\n",
        "from t5x.examples.t5 import network\n",
        "import t5x\n",
        "from t5x import models\n",
        "from t5x import partitioning\n",
        "from t5x import trainer as trainer_lib\n",
        "from t5x import utils\n",
        "from t5x.infer import _extract_tokens_and_aux_values\n",
        "from t5x.infer import _Inferences\n",
        "from t5x.interactive_model import InteractiveModel\n",
        "from t5x.interactive_model import get_batches_from_seqio\n",
        "from t5x.interactive_model import get_dataset_from_natural_text_examples\n",
        "from t5x.interactive_model import get_gin_config_from_interactive_model\n",
        "from t5x.interactive_model import T5XScriptType\n",
        "from t5x.interactive_model import InferenceType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Lb-Z1fkF5a"
      },
      "source": [
        "Before we begin, let's initialize instances of the constructor arguments for the `InteractiveModel`. As mentioned previously, this will enable us to dive into how the `InteractiveModel` runs inference.\n",
        "\n",
        "If you don't understand the lines of code below, or have questions about how to initialize these parameters, please see the [first Colab](https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/introduction.ipynb) in this tutorial series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne8U8qoWkX_r"
      },
      "outputs": [],
      "source": [
        "# Define a model. The configuration below corresponds to the T5 1.1 Small model.\n",
        "t5_config = network.T5Config(\n",
        "    vocab_size=32128,\n",
        "    dtype='bfloat16',\n",
        "    emb_dim=512,\n",
        "    num_heads=6,\n",
        "    num_encoder_layers=8,\n",
        "    num_decoder_layers=8,\n",
        "    head_dim=64,\n",
        "    mlp_dim=1024,\n",
        "    mlp_activations=('gelu', 'linear'),\n",
        "    dropout_rate=0.0,\n",
        "    logits_via_embedding=False)\n",
        "module = network.Transformer(config=t5_config)\n",
        "model = t5x.models.EncoderDecoderModel(\n",
        "    module=module,\n",
        "    input_vocabulary=t5.data.get_default_vocabulary(),\n",
        "    output_vocabulary=t5.data.get_default_vocabulary(),\n",
        "    optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0))\n",
        "# Define checkpoint arguments.\n",
        "checkpoint_path='gs://t5-data/pretrained_models/cbqa/small_ssm_nq/model.ckpt-1110000'\n",
        "dtype='bfloat16'\n",
        "restore_mode='specific'\n",
        "# Define a partitioner.\n",
        "partitioner=partitioning.PjitPartitioner(num_partitions=2)\n",
        "# Define additional, miscellaneous constructor arguments.\n",
        "batch_size=8\n",
        "task_feature_lengths = {'inputs': 38, 'targets': 18}\n",
        "output_dir='/tmp/output_dir'\n",
        "input_shapes = {\n",
        "    'encoder_input_tokens': np.array([8, 38]),\n",
        "    'decoder_target_tokens': np.array([8, 18]),\n",
        "    'decoder_input_tokens': np.array([8, 18]),\n",
        "    'decoder_loss_weights': np.array([8, 18])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYwdg-fFTU8Q"
      },
      "source": [
        "In addition, we will run all code that is performed when we initialize the InteractiveModel. If you don't understand the lines of code below or have any additional questions about how/why we do the steps below, please see the [second Colab](https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/training.ipynb) in this tutorial series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmGTJBAcTpMR"
      },
      "outputs": [],
      "source": [
        "# 1.) Configure the Output Directory\n",
        "output_dir = re.sub(r\"(?<!gs:)([\\/]{2,})\", \"/\", output_dir)\n",
        "if not os.path.exists(output_dir):\n",
        "  os.mkdir(output_dir)\n",
        "\n",
        "# 2.) Initialize RNGs\n",
        "init_random_seed = 42\n",
        "random_seed = multihost_utils.broadcast_one_to_all(np.int32(init_random_seed))\n",
        "utils.set_hardware_rng_ops()\n",
        "rng = random.PRNGKey(random_seed)\n",
        "init_rng, trainer_rng = random.split(rng, 2)\n",
        "\n",
        "# 3.) Validate the Partitioner\n",
        "if partitioner._model_parallel_submesh:\n",
        "  num_partitions = np.prod(partitioner._model_parallel_submesh)\n",
        "else:\n",
        "  num_partitions = partitioner._num_partitions\n",
        "if jax.device_count() % num_partitions != 0:\n",
        "  raise ValueError(\n",
        "    \"The number of devices available must be a multiple of the number of\",\n",
        "    f\" partitions. There are {jax.device_count()} devices available, but\",\n",
        "    f\" the number of partitions is set to {num_partitions}. Please\",\n",
        "    \" provide a different number of partitions.\")\n",
        "\n",
        "# 4.) Create a Checkpoint Manager\n",
        "# a.) Define CheckpointCfg wrappers.\n",
        "save_checkpoint_cfg = utils.SaveCheckpointConfig(\n",
        "        dtype=dtype,\n",
        "        keep=5, # The number of checkpoints to keep in the output_dir.\n",
        "        save_dataset=False)\n",
        "restore_checkpoint_cfg = utils.RestoreCheckpointConfig(\n",
        "        dtype=dtype,\n",
        "        mode=restore_mode,\n",
        "        path=checkpoint_path)\n",
        "\n",
        "# b.) Define a train state initializer, which will help us get information about the\n",
        "# TrainState shape.\n",
        "train_state_initializer = utils.TrainStateInitializer(\n",
        "        optimizer_def=model.optimizer_def,\n",
        "        init_fn=model.get_initial_variables,\n",
        "        input_shapes=input_shapes,\n",
        "        input_types=None,\n",
        "        partitioner=partitioner)\n",
        "\n",
        "# c.) Define the checkpoint manager.\n",
        "checkpoint_manager = utils.LegacyCheckpointManager(\n",
        "        save_cfg=save_checkpoint_cfg,\n",
        "        restore_cfg=restore_checkpoint_cfg,\n",
        "        train_state_shape=train_state_initializer.global_train_state_shape,\n",
        "        partitioner=partitioner,\n",
        "        ds_iter=None,\n",
        "        model_dir=output_dir)\n",
        "\n",
        "### 5.) Restore the Model from a Checkpoint, or Initialize from Scratch ###\n",
        "def get_state(rng):\n",
        "  return train_state_initializer.from_scratch(rng).state_dict()\n",
        "\n",
        "# a.) Try to restore a model from a checkpoint.\n",
        "train_state = checkpoint_manager.restore(\n",
        "  [restore_checkpoint_cfg.path],\n",
        "  restore_checkpoint_cfg,\n",
        "  utils.get_fallback_state(restore_checkpoint_cfg, get_state, init_rng)\n",
        ")\n",
        "\n",
        "# b.) If no checkpoint to restore, init from scratch.\n",
        "if train_state is None:\n",
        "  train_state = train_state_initializer.from_scratch(init_rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib9aOi2xaCKQ"
      },
      "source": [
        "# Inference Deep Dive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANqpfv0lAVqL"
      },
      "source": [
        "**Defining a Batch of Examples to Run Inference On**\\\n",
        "Let's start by defining a batch of examples that we will get predictions and scores for.\n",
        "\n",
        "These examples should be a list of inputs; we don't need any targets, because we will eventually generate predictions. For this Colab, we'll use a set of natural text questions (and we will generate the answers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhhR0yDcAn7w"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    b'nq question: who has been appointed as the new chairman of sebi',\n",
        "    b'nq question: who wrote the book lion the witch and the wardrobe',\n",
        "    b'nq question: how many planes did japan lose at pearl harbor',\n",
        "    b'nq question: who does the voice of mcgruff the dog',\n",
        "    b'nq question: who sings the wheels in the sky keep on turning',\n",
        "    b'nq question: who voices regina in glitter force doki doki',\n",
        "    b'nq question: when did the us become allies with britain',\n",
        "    b'nq question: who won the rugby 7 in las vegas'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYV1LMS5taE9"
      },
      "source": [
        "We also define the required features of the examples. For this Colab, we will only require an `inputs` and `targets` entry, as defined below. `targets` will be empty for our examples, because we do not have any targets to provide at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5I7YMotb9U"
      },
      "outputs": [],
      "source": [
        "output_features = {\n",
        "        \"inputs\":\n",
        "            seqio.Feature(\n",
        "                vocabulary=model.input_vocabulary, add_eos=True),\n",
        "        \"targets\":\n",
        "            seqio.Feature(\n",
        "                vocabulary=model.output_vocabulary, add_eos=True)\n",
        "    }\n",
        "features = dict(sorted(output_features.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZuleutswi1H"
      },
      "source": [
        "Finally, we'll have to determine whether we want to get predictions or scores for this batch. For this example, we'll get predictions, which we'll denote by setting an inference mode variable to `PREDICT_WITH_AUX`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbMPt5eBw-a4"
      },
      "outputs": [],
      "source": [
        "mode = InferenceType.PREDICT_WITH_AUX\n",
        "# Try replacing this variable with `InferenceType.SCORE` to produce scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mixLzcBkQOT_"
      },
      "source": [
        "Now, let's break down what the interactive model does to run inference.\n",
        "\n",
        "The `InteractiveModel` `infer_with_preprocessors()` method only performs three actions:\n",
        "\n",
        "\n",
        "1.   Convert the natural text examples into a tf.Dataset.\n",
        "2.   Define an `infer_fn`; depending on whether we want predictions or scores, this function will be equivalent to `model.predict_batch` or `model.score_batch`.\n",
        "3.   Extract inferences and return them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug0zJx2kQk6g"
      },
      "source": [
        "**Prepare the dataset** \\\n",
        "\n",
        "Preparing the data for inference is fairly straightforward; in fact, this is nearly the same data preparation that happens for training.\n",
        "\n",
        "First, we convert the natural text examples into a tf.Dataset and run any preprocessors; T5X has a helper function, `get_dataset_from_natural_text_examples`, that can do exactly that. For this example, the only preprocessing we will do is tokenization and appending an EOS token. If you are interested in learning more about preprocessors, please take a look at https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.mdx-colab-intro.\n",
        "\n",
        "Finally, we  convert all features using the model's feature converter, pad all batches of data, and define an iterator over our data (this allows us to run inference on multiple batches of examples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chPomDFxQ6r3"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset_from_natural_text_examples(\n",
        "    examples,\n",
        "    preprocessors=[\n",
        "        seqio.preprocessors.tokenize,\n",
        "        seqio.preprocessors.append_eos\n",
        "    ],\n",
        "    task_feature_lengths=task_feature_lengths,\n",
        "    features=features)\n",
        "feature_converter = model.FEATURE_CONVERTER_CLS(pack=False)\n",
        "model_dataset = feature_converter(\n",
        "    dataset, task_feature_lengths=task_feature_lengths)\n",
        "# Zip task and model features.\n",
        "infer_dataset = tf.data.Dataset.zip((dataset, model_dataset))\n",
        "# Create batches and index them.\n",
        "infer_dataset = infer_dataset.padded_batch(\n",
        "    batch_size, drop_remainder=False).enumerate()\n",
        "infer_dataset_iter: Iterator[Tuple[int, Any]] = iter(\n",
        "    infer_dataset.prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toq2uz7dAfdL"
      },
      "source": [
        "**Define Infer Function** \\\n",
        "\n",
        "We'll define a helper function that runs inference on a single batch, making it easy to loop over this helper and run inference for multiple batches. This `infer_fn` can either get predictions or scores, depending on the mode we've previously set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv8RXHeXK6mk"
      },
      "outputs": [],
      "source": [
        "if mode == InferenceType.PREDICT_WITH_AUX:\n",
        "  infer_step = model.predict_batch_with_aux\n",
        "elif mode == InferenceType.SCORE:\n",
        "  infer_step = model.score_batch\n",
        "else:\n",
        "  raise ValueError(\"Mode must be `predict_with_aux`, or `score`,\"\n",
        "                  f\" but instead was {mode}.\")\n",
        "infer_fn = functools.partial(\n",
        "  utils.get_infer_fn(\n",
        "    infer_step=infer_step,\n",
        "    batch_size=batch_size,\n",
        "    train_state_axes=train_state_initializer.train_state_axes,\n",
        "    partitioner=partitioner),\n",
        "  train_state=train_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-BoczLLzaGb"
      },
      "source": [
        "**Extract Inferences** \\\n",
        "\n",
        "Finally, we will extract inferences for each batch of examples provided. For each batch, we:\n",
        "\n",
        "1.  Unzip the dataset to get both the task dataset and the model dataset (the model dataset is what you get when you've passed the task dataset through the model feature converter).\n",
        "2.  Get an RNG for the batch.\n",
        "3.  Extract predictions and auxiliary values using the T5X helper, `_extract_tokens_and_aux_values`.\n",
        "4.  Decode the predictions using our vocabulary.\n",
        "5.  Accumulate predictions, aux values, and inputs across all of our batches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDAyEz9mzghq"
      },
      "outputs": [],
      "source": [
        "# Main Loop over \"batches\".\n",
        "all_inferences = []\n",
        "all_aux_values = {}\n",
        "for chunk, chunk_batch in infer_dataset_iter:\n",
        "  # Load the dataset for the next chunk. We can't use `infer_dataset_iter`\n",
        "  # directly since `infer_fn` needs to know the exact size of each chunk,\n",
        "  # which may be smaller for the final one.\n",
        "  chunk_dataset = tf.data.Dataset.from_tensor_slices(chunk_batch)\n",
        "  chunk_dataset.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Unzip chunk dataset in to pretokenized and model datasets.\n",
        "  task_dataset = chunk_dataset.map(\n",
        "      lambda p, m: p, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  model_dataset = chunk_dataset.map(\n",
        "      lambda p, m: m, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Get a chunk-specific RNG key.\n",
        "  chunk_rng = jax.random.fold_in(jax.random.PRNGKey(0), chunk)\n",
        "\n",
        "  inferences = _extract_tokens_and_aux_values(\n",
        "      infer_fn(model_dataset.enumerate(), rng=chunk_rng))\n",
        "\n",
        "  predictions, aux_values = inferences\n",
        "  accumulated_inferences = []\n",
        "  for idx, inputs in task_dataset.enumerate().as_numpy_iterator():\n",
        "    prediction = predictions[idx]\n",
        "    # Decode predictions if applicable.\n",
        "    if mode == InferenceType.PREDICT_WITH_AUX:\n",
        "      prediction = features[\"targets\"].vocabulary.decode_tf(\n",
        "          tf.constant(prediction)).numpy()\n",
        "    accumulated_inferences.append((inputs, prediction))\n",
        "  all_inferences += accumulated_inferences\n",
        "  # Accumulate aux values over batches.\n",
        "  if not all_aux_values:\n",
        "    all_aux_values = aux_values\n",
        "  else:\n",
        "    for key, values in aux_values.items():\n",
        "      all_aux_values[key] += values\n",
        "print(all_inferences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIzrCUyWQcDZ"
      },
      "source": [
        "We can parse these predictions into a more readable format using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9_BPXG_QgJs"
      },
      "outputs": [],
      "source": [
        "for input, prediction in all_inferences:\n",
        "  print(f\"Input: {input['inputs_pretokenized']}\")\n",
        "  print(f\"Prediction: {prediction}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB0U_kfRNIyR"
      },
      "source": [
        "The code snippets above exactly replicate the `InteractiveModel` `infer_with_preprocessors()` method (see [source code](https://github.com/google-research/t5x/blob/main/t5x/interactive_model.py)); running the code snippets above is exactly equivalent to running `interactive_model.infer_with_preprocessors(mode, examples, preprocessors=[seqio.preprocessors.tokenize, seqio.preprocessors.append_eos])`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcDwmp_AxnOG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QR5LnmN4ikp"
      },
      "source": [
        "# Advanced Topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLstCKpP8Ge7"
      },
      "source": [
        "## T5X Inference Binaries and Other Advanced Features\n",
        "\n",
        "T5X offers inference binaries that have the same functionality as the InteractiveModel, with additional features as well (more advanced compiling, inference on TF Example files, prediction services, etc.). Importantly, these binaries are configured using [Gin](https://github.com/google/gin-config/blob/main/README.md); if you are not familiar with Gin, please take a look at this [Gin Primer](https://github.com/google-research/t5x/blob/main/docs/usage.md/gin) to get started.\n",
        "\n",
        "If you are familiar with Gin and interested in using the T5X inference binaries, we have provided a helper function, get_gin_config_from_interactive_model, which will take an InteractiveModel instance and generate the gin config that you can use to run the T5X inference binaries; this gin config will exactly reproduce the InteractiveModel inference functionality we've described above. We've provided an example below.\n",
        "\n",
        "Importantly, the InteractiveModel takes in a model, partitioner, and data, so we cannot generate Gin configs for these components. You can pass Gin config strings for the model and partitioner components to the helper function, as demonstrated below. Additionally, you can pass a SeqIO task containing your data to the helper function. See the section below if you are unfamiliar with SeqIO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqa49ZpjnRN1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/google-research/google-research.git google_research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhgUZ0w6yQsE"
      },
      "outputs": [],
      "source": [
        "# Define an InteractiveModel instance, based on the `small` T5X EncoderDecoder model.\n",
        "t5_config = network.T5Config(\n",
        "    vocab_size=32128,\n",
        "    dtype='bfloat16',\n",
        "    emb_dim=512,\n",
        "    num_heads=6,\n",
        "    num_encoder_layers=8,\n",
        "    num_decoder_layers=8,\n",
        "    head_dim=64,\n",
        "    mlp_dim=1024,\n",
        "    mlp_activations=('gelu', 'linear'),\n",
        "    dropout_rate=0.0,\n",
        "    logits_via_embedding=False)\n",
        "module = network.Transformer(config=t5_config)\n",
        "model = t5x.models.EncoderDecoderModel(\n",
        "    module=module,\n",
        "    input_vocabulary=t5.data.get_default_vocabulary(),\n",
        "    output_vocabulary=t5.data.get_default_vocabulary(),\n",
        "    optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0),\n",
        "    decode_fn=functools.partial(\n",
        "        t5x.decoding.temperature_sample, temperature=1.0, topk=40))\n",
        "interactive_model = InteractiveModel(\n",
        "    batch_size=8,\n",
        "    task_feature_lengths={'inputs': 38, 'targets': 18},\n",
        "    output_dir='/tmp/output_dir',\n",
        "    partitioner=partitioning.PjitPartitioner(\n",
        "      num_partitions=1,\n",
        "      model_parallel_submesh=None,\n",
        "      logical_axis_rules=partitioning.standard_logical_axis_rules()),\n",
        "    model=model,\n",
        "    dtype='bfloat16',\n",
        "    restore_mode='specific',\n",
        "    checkpoint_path='gs://t5-data/pretrained_models/cbqa/small_ssm_nq/model.ckpt-1110000',\n",
        "    input_shapes={\n",
        "      'encoder_input_tokens': np.array([8, 38]),\n",
        "      'decoder_target_tokens': np.array([8, 18]),\n",
        "      'decoder_input_tokens': np.array([8, 18]),\n",
        "      'decoder_loss_weights': np.array([8, 18])\n",
        "    },\n",
        "    input_types=None)\n",
        "\n",
        "# Define Gin Config strings for the model, partitioner, and any imports.\n",
        "imports_str = \"\"\"from t5x import models\n",
        "from t5x import partitioning\n",
        "import t5.data.mixtures\n",
        "include 't5x/examples/t5/t5_1_1/small.gin'\n",
        "\n",
        "# Register necessary SeqIO Tasks/Mixtures.\n",
        "import google_research.t5_closed_book_qa.t5_cbqa.tasks\"\"\"\n",
        "partitioner_config = 'partitioning.PjitPartitioner.num_partitions = 2'\n",
        "\n",
        "gin_config_str = get_gin_config_from_interactive_model(\n",
        "  interactive_model=interactive_model,\n",
        "  script_type=T5XScriptType.INFERENCE,\n",
        "  task_name='closed_book_qa',\n",
        "  partitioner_config_str=partitioner_config,\n",
        "  model_config_str='',  # No config needed, since we just import the model.\n",
        "  imports_str=imports_str,\n",
        ")\n",
        "print(gin_config_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGd1DxDT3gB7"
      },
      "source": [
        "Once you have generated the `gin_config_str` as above, you can write this string to a file and launch your inference experiment locally by running the following on commandline:\n",
        "\n",
        "\n",
        "```\n",
        "INFER_OUTPUT_DIR=\"/tmp/inference-model/\"\n",
        "python -m t5x.infer_unfragmented \\\n",
        "  --gin_file=${GIN_FILE_PATH} \\\n",
        "  --gin.INFER_OUTPUT_DIR=\\\"${INFER_OUTPUT_DIR}\\\" \\\n",
        "  --alsologtostderr\n",
        "```\n",
        "For more details on inference using the T5X inference binaries, please see the [Inference](https://github.com/google-research/t5x/blob/main/docs/usage.md/infer-seqio) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi29fMdv4mSr"
      },
      "source": [
        "## SeqIO\n",
        "\n",
        "If you are interested in T5X, you may also be interested in, or have heard of, SeqIO. SeqIO is a library for processing sequential data to be fed into downstream sequence models. At a high level, SeqIO relies on user-defined `Tasks` and `Mixtures` that can be used to retrieve and evaluate datasets.\n",
        "\n",
        "We won't go into details about SeqIO here; we recommend checking out this [SeqIO Introductory guide](https://github.com/google/seqio/blob/main/README.md/index) and/or clicking below to run a SeqIO Introductory Colab. The rest of this section will assume a basic understanding of SeqIO.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/google-research/seqio/blob/main/seqio/notebooks/Basics_Task_and_Mixtures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "If you are already familiar with SeqIO and have a SeqIO task/mixture that you would like to use in this Colab, we do provide a SeqIO bridge that takes in a SeqIO task/mixture and produces batches of examples that can be processed by the code snippets above. We've provided an example of this bridge below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "j4ASuQHAk_LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM0nRIEFwyj_"
      },
      "outputs": [],
      "source": [
        "import google_research.t5_closed_book_qa.t5_cbqa.tasks\n",
        "batches = get_batches_from_seqio(\n",
        "        task_or_mixture_name='natural_questions_open',\n",
        "        split='validation',\n",
        "        batch_size=8,\n",
        "        num_batches=2,\n",
        "        seed=42)\n",
        "print(f\"Batches: {batches}\")\n",
        "# Train the interactive model on the provided batches.\n",
        "original_step = interactive_model.step\n",
        "_ = interactive_model.train_loop(num_steps=len(batches), train_batches=batches)\n",
        "print(f\"Original Step: {original_step}, Current Step: {interactive_model.step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elt08160w03X"
      },
      "source": [
        "The `get_batches_from_seqio` bridge can take several constructor arguments:\n",
        "\n",
        "\n",
        "1.   `task_or_mixture_name`: the name of the SeqIO task/mixture to read data from. It should be noted that your task/mixture must already be registered with SeqIO, and you must import the module that defines your task/mixture here (as seen above).\n",
        "2.   `split`: the split of the Task/Mixture to read data from.\n",
        "3.   `batch_size`: how many examples should appear in each batch.\n",
        "4.   `num_batches`: the total number of batches to return.\n",
        "5.   `get_pretokenized_examples`: optional. A boolean, defaulting to True, that determines whether we should read the `inputs_pretokenized`/`targets_pretokenized` elements from an example, or the `inputs`/`targets` elements. \\\n",
        "The `train_step`, `predict`, `predict_with_aux`, `score`, and `evaluate` methods of the InteractiveModel assume that we should run [tokenization](https://github.com/google/seqio/tree/main/seqio/preprocessors.py) and [appending an EOS token](https://github.com/google/seqio/tree/main/seqio/preprocessors.py) as the only preprocessors. To use these methods with this pre-defined list of preprocessors, you can set `get_pretokenized_examples=True` to retrieve examples that still need to be tokenized, and these InteractiveModel methods will handle running these preprocessors. This setting can also be helpful if you want to inspect the natural text inputs/targets of your SeqIO task. \\\n",
        "However, some SeqIO tasks do not use tokenization (ex: span corruption). You can set `get_pretokenized_examples=False`, and this bridge will read the fully preprocessed examples from the SeqIO task. You can then run `train_step_with_preprocessors`, `infer_with_preprocessors`, or `evaluate_with_preprocessors` and provide an empty preprocessors list (because all preprocessing has already been completed by this bridge) to run training/inference/evaluation. We have provided an example of using this bridge to retrieve fully preprocessed examples below.\n",
        "6.   `sequence_length`: optional. A dictionary mapping feature key to maximum length (int) for that feature. Used by SeqIO to retrieve the dataset/examples.\n",
        "7.   `**get_dataset_kwargs`: there are many [additional parameters](https://github.com/google/seqio/tree/main/seqio/dataset_providers.py) that can be set in the `SeqIO.get_dataset` function. If you would like to set any of these arguments, you can set them using this `kwargs` parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjKBCX39w0Xl"
      },
      "outputs": [],
      "source": [
        "import t5.data.tasks\n",
        "batches = get_batches_from_seqio(\n",
        "    task_or_mixture_name='c4_v220_span_corruption',\n",
        "    split='validation',\n",
        "    batch_size=8,\n",
        "    num_batches=1,\n",
        "    get_pretokenized_examples=False,\n",
        "    sequence_length=interactive_model._task_feature_lengths,\n",
        "    seed=42)\n",
        "batch = batches[0]  # We expect only a single batch.\n",
        "original_step = interactive_model.step\n",
        "interactive_model.train_step_with_preprocessors(\n",
        "        examples=batch, preprocessors=[])\n",
        "print(f\"Original Step: {original_step}, Current Step: {interactive_model.step}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6HnnTB9km7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e0a8923"
      },
      "source": [
        "# Task\n",
        "Explain how to adapt the provided T5X notebook for information retrieval by encoding documents on the fly and scoring them. Detail the necessary code modifications for the model, task, data processing, and inference mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "539da473"
      },
      "source": [
        "## Understand information retrieval with t5x\n",
        "\n",
        "### Subtask:\n",
        "Briefly explain how T5X can be used for information retrieval, focusing on encoding and scoring documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cf1c617"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires explaining how T5X can be used for information retrieval by encoding and scoring documents. This can be done by describing the encoding process to create embeddings and how the `score_batch` function is used for scoring. This explanation can be provided in a markdown cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "T5X can be adapted for information retrieval by leveraging its ability to encode text into numerical representations, known as embeddings. In this approach, documents are passed through the T5X encoder to obtain fixed-size vector representations. These embeddings capture the semantic meaning of the documents.\n",
        "\n",
        "For information retrieval, a query is also encoded into a similar embedding using the T5X encoder. The relevance between the query and a document can then be determined by calculating a similarity score between their respective embeddings. Common similarity measures include cosine similarity.\n",
        "\n",
        "T5X provides functionalities like the `score_batch` function, which can be used to compute scores between input sequences and target sequences. In an information retrieval context, this function can be adapted to score the relevance of a document embedding with respect to a query embedding. While `score_batch` typically computes likelihoods, it can be modified or used in conjunction with other components to produce or utilize similarity scores for retrieval."
      ],
      "metadata": {
        "id": "aBJase6QspZs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGCmfg-bswiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54e218c7"
      },
      "source": [
        "## Modified Code for Information Retrieval\n",
        "\n",
        "Below are new cells containing the code modifications discussed for adapting this notebook to information retrieval. These cells replace or modify the functionality of the original cells to focus on encoding and scoring documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e5f9d9d"
      },
      "source": [
        "### Modified Model and Data Configuration\n",
        "\n",
        "This cell modifies the model definition, input shapes, and task feature lengths to be more suitable for document encoding and scoring. **You may need to adjust the `t5_config` and model class based on the specific T5X model you intend to use for document encoding.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "239186de"
      },
      "source": [
        "# Define a model suitable for encoding documents.\n",
        "# We'll use the T5 1.1 Small configuration as a starting point, configured as encoder-only.\n",
        "t5_ir_config = network.T5Config(\n",
        "    vocab_size=32128,\n",
        "    dtype='bfloat16',\n",
        "    emb_dim=512,\n",
        "    num_heads=6,\n",
        "    num_encoder_layers=8,\n",
        "    num_decoder_layers=0, # Explicitly set to 0 for encoder-only\n",
        "    head_dim=64,\n",
        "    mlp_dim=1024,\n",
        "    mlp_activations=('gelu', 'linear'),\n",
        "    dropout_rate=0.0,\n",
        "    logits_via_embedding=False) # Closing parenthesis added here\n",
        "\n",
        "# Using Transformer module with encoder-only configuration\n",
        "#module_ir = network.Transformer(config=t5_ir_config)\n",
        "\n",
        "# Using EncoderDecoderModel but it functions as encoder-only due to config.\n",
        "# Note: For pure encoder models, a dedicated EncoderOnlyModel class would be ideal\n",
        "# if available in the T5X library. Using EncoderDecoderModel with num_decoder_layers=0\n",
        "# is a common workaround, but might still have compatibility issues with full checkpoints.\n",
        "model_ir = t5x.models.EncoderDecoderModel(\n",
        "    module=module_ir,\n",
        "    input_vocabulary=t5.data.get_default_vocabulary(),\n",
        "    output_vocabulary=t5.data.get_default_vocabulary(),\n",
        "    optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0))\n",
        "\n",
        "\n",
        "# Define checkpoint arguments for your IR model.\n",
        "# Using a publicly available T5 1.1 Small checkpoint as an example.\n",
        "# IMPORTANT: This checkpoint is for a full T5 model. Loading it into an\n",
        "# encoder-only configuration might require careful handling of the state\n",
        "# dictionary to only load encoder parameters. If this causes issues, you\n",
        "# might need a checkpoint specifically trained for an encoder-only model\n",
        "# or implement custom state loading logic.\n",
        "checkpoint_path_ir = 'gs://t5-data/pretrained_models/t5x/retrieval/gtr_base/checkpoint_1819900/'\n",
        "dtype_ir = 'bfloat16'\n",
        "restore_mode_ir = 'specific'\n",
        "\n",
        "# Define a partitioner. Adjust based on your setup.\n",
        "partitioner_ir = partitioning.PjitPartitioner(num_partitions=2) # Adjust num_partitions\n",
        "\n",
        "# Define batch size and task feature lengths for your document data.\n",
        "# Adjust these based on the maximum sequence length of your documents.\n",
        "batch_size_ir = 8 # Adjust batch size\n",
        "# Adjust input length based on expected document length. Targets can be minimal for scoring.\n",
        "# For encoding, only 'inputs' is strictly necessary for the model input shape.\n",
        "# However, the InteractiveModel and feature converter might expect 'targets' as well.\n",
        "document_feature_lengths = {'inputs': 128, 'targets': 1}\n",
        "\n",
        "# Define input shapes for your IR model.\n",
        "# These shapes are based on the model's expected inputs for the forward pass.\n",
        "input_shapes_ir = {\n",
        "    'encoder_input_tokens': np.array([batch_size_ir, document_feature_lengths['inputs']]),\n",
        "    'decoder_target_tokens': np.array([batch_size_ir, document_feature_lengths['targets']]), # Needed for InteractiveModel structure\n",
        "    'decoder_input_tokens': np.array([batch_size_ir, document_feature_lengths['targets']]), # Needed for InteractiveModel structure\n",
        "    'decoder_loss_weights': np.array([batch_size_ir, document_feature_lengths['targets']]) # Needed for InteractiveModel structure\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56190e88"
      },
      "source": [
        "### Modified InteractiveModel Initialization\n",
        "\n",
        "This cell re-initializes the `InteractiveModel` with the configuration for information retrieval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "932a1126"
      },
      "source": [
        "# Re-initialize InteractiveModel with IR configuration\n",
        "interactive_model_ir = InteractiveModel(\n",
        "    batch_size=batch_size_ir,\n",
        "    task_feature_lengths=document_feature_lengths,\n",
        "    output_dir='/tmp/ir_output_dir', # Consider a new output directory\n",
        "    partitioner=partitioner_ir,\n",
        "    model=model_ir,\n",
        "    dtype=dtype_ir,\n",
        "    restore_mode=restore_mode_ir, # This will be set to 'allow_partial' below\n",
        "    checkpoint_path=checkpoint_path_ir,\n",
        "    input_shapes=input_shapes_ir,\n",
        "    input_types=None)\n",
        "\n",
        "# Initialize RNGs and Checkpoint Manager for the IR model\n",
        "output_dir_ir = re.sub(r\"(?<!gs:)([\\/]{2}{2})\", \"/\", '/tmp/ir_output_dir')\n",
        "if not os.path.exists(output_dir_ir):\n",
        "  os.mkdir(output_dir_ir)\n",
        "\n",
        "init_random_seed_ir = 42\n",
        "random_seed_ir = multihost_utils.broadcast_one_to_all(np.int32(init_random_seed_ir))\n",
        "utils.set_hardware_rng_ops() # This might be redundant if already called\n",
        "rng_ir = random.PRNGKey(random_seed_ir)\n",
        "init_rng_ir, _ = random.split(rng_ir, 2) # We only need init_rng for restoring\n",
        "\n",
        "save_checkpoint_cfg_ir = utils.SaveCheckpointConfig(\n",
        "        dtype=dtype_ir,\n",
        "        keep=5,\n",
        "        save_dataset=False)\n",
        "restore_checkpoint_cfg_ir = utils.RestoreCheckpointConfig(\n",
        "        dtype=dtype_ir,\n",
        "        mode='allow_partial', # Changed restore_mode to 'allow_partial'\n",
        "        path=checkpoint_path_ir)\n",
        "\n",
        "train_state_initializer_ir = utils.TrainStateInitializer(\n",
        "        optimizer_def=model_ir.optimizer_def,\n",
        "        init_fn=model_ir.get_initial_variables,\n",
        "        input_shapes=input_shapes_ir,\n",
        "        input_types=None,\n",
        "        partitioner=partitioner_ir)\n",
        "\n",
        "checkpoint_manager_ir = utils.LegacyCheckpointManager(\n",
        "        save_cfg=save_checkpoint_cfg_ir,\n",
        "        restore_cfg=restore_checkpoint_cfg_ir,\n",
        "        train_state_shape=train_state_initializer_ir.global_train_state_shape,\n",
        "        partitioner=partitioner_ir,\n",
        "        ds_iter=None,\n",
        "        model_dir=output_dir_ir)\n",
        "\n",
        "def get_state_ir(rng):\n",
        "  return train_state_initializer_ir.from_scratch(rng).state_dict()\n",
        "\n",
        "# a.) Try to restore a model from a checkpoint.\n",
        "# Use the modified restore_checkpoint_cfg_ir with 'allow_partial'\n",
        "train_state_ir = checkpoint_manager_ir.restore(\n",
        "  [restore_checkpoint_cfg_ir.path],\n",
        "  restore_checkpoint_cfg_ir,\n",
        "  utils.get_fallback_state(restore_checkpoint_cfg_ir, get_state_ir, init_rng_ir)\n",
        ")\n",
        "\n",
        "\n",
        "# b.) If no checkpoint to restore, init from scratch.\n",
        "if train_state_ir is None:\n",
        "  print(\"Warning: No checkpoint found for IR model. Initializing from scratch.\")\n",
        "  train_state_ir = train_state_initializer_ir.from_scratch(init_rng_ir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88c1c180"
      },
      "source": [
        "### Prepare Document Data for Scoring\n",
        "\n",
        "This cell shows how to prepare your document data and queries for scoring. **You will need to replace the placeholder data with your actual document and query loading and preprocessing logic.** This might involve reading text files, tokenizing, and formatting them according to your model's requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66e0a81d"
      },
      "source": [
        "### Conceptual Code for Loading Only Encoder Parameters\n",
        "\n",
        "This cell provides a conceptual example of how you might use `state_transformation_fns` to load only the encoder parameters from a full T5 checkpoint. **This is an advanced technique and the code provided is a template that will likely need significant adaptation based on the specific structure of your model and the checkpoint.**\n",
        "\n",
        "You would typically use this with a more modern checkpointing utility like `orbax.checkpoint.Checkpointer` instead of `LegacyCheckpointManager`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "000c8e0d"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax.traverse_util import flatten_dict, unflatten_dict\n",
        "from flax.core.frozen_dict import FrozenDict\n",
        "\n",
        "# This is a conceptual example of a state transformation function.\n",
        "# You will need to adapt this based on the exact structure of your checkpoint\n",
        "# and your encoder-only model's expected state dictionary.\n",
        "def remove_decoder_params(source_state_dict):\n",
        "    \"\"\"Removes decoder parameters from a full T5 state dictionary.\"\"\"\n",
        "    # Flatten the dictionary for easier manipulation\n",
        "    flat_state_dict = flatten_dict(source_state_dict)\n",
        "\n",
        "    # Create a new dictionary with only encoder-related keys\n",
        "    # The exact key paths will depend on the checkpoint structure\n",
        "    encoder_flat_state_dict = {\n",
        "        key: value for key, value in flat_state_dict.items()\n",
        "        if key[0] == 'encoder' # Assuming 'encoder' is the top-level key for encoder params\n",
        "    }\n",
        "\n",
        "    # Unflatten the dictionary back to its nested structure\n",
        "    # This might require recreating the expected nested structure for the encoder\n",
        "    # based on your model definition. This part is complex and highly dependent\n",
        "    # on your model.\n",
        "    # This is a simplified example and might not work directly.\n",
        "    transformed_state_dict = unflatten_dict(encoder_flat_state_dict)\n",
        "\n",
        "    # You might need additional logic here to ensure the structure matches\n",
        "    # the expected state dictionary of your encoder-only model.\n",
        "\n",
        "    return transformed_state_dict\n",
        "\n",
        "# Example usage with a hypothetical Checkpointer (not directly usable with LegacyCheckpointManager)\n",
        "# Assuming you have a Checkpointer instance and a checkpoint path:\n",
        "# checkpointer = orbax.checkpoint.Checkpointer(...)\n",
        "# checkpoint_path = 'gs://t5-data/pretrained_models/t5_1_1_small/checkpoint_1000000'\n",
        "# target_state = train_state_initializer_ir.from_scratch(init_rng_ir).state_dict() # Get the structure of your target state\n",
        "\n",
        "# try:\n",
        "#     restored_state = checkpointer.restore(\n",
        "#         checkpoint_path,\n",
        "#         item=target_state, # Provide the target state structure\n",
        "#         state_transformation_fns={'params': remove_decoder_params} # Apply the transformation\n",
        "#     )\n",
        "# except Exception as e:\n",
        "#     print(f\"Error during restoration with transformation: {e}\")\n",
        "#     restored_state = None\n",
        "\n",
        "# If successful, restored_state would contain only the loaded encoder parameters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5a98c33"
      },
      "source": [
        "# Replace with your actual document data and queries\n",
        "# This is a placeholder\n",
        "documents = [\n",
        "    b'document 1: This is the content of the first document.',\n",
        "    b'document 2: The second document contains different information.',\n",
        "    b'document 3: A third document for testing purposes.',\n",
        "    b'document 4: Yet another document.',\n",
        "    b'document 5: Content for the fifth document.',\n",
        "    b'document 6: More text for document six.',\n",
        "    b'document 7: Document number seven.',\n",
        "    b'document 8: The eighth document content.'\n",
        "]\n",
        "\n",
        "query = b'query: information about documents' # Replace with your query\n",
        "\n",
        "# Combine query and documents for scoring batch.\n",
        "# The exact structure will depend on how your model expects input for scoring.\n",
        "# A common approach is to concatenate query and document representations.\n",
        "# Here's a conceptual example:\n",
        "# You'll likely need a custom preprocessing function here.\n",
        "# This is a simplified representation.\n",
        "scoring_examples = []\n",
        "for doc in documents:\n",
        "    # Create example pairs of (query, document) for scoring\n",
        "    # The exact format depends on your model's scoring input requirement\n",
        "    # For a simple scoring setup using EncoderDecoderModel's score_batch:\n",
        "    # Inputs could be the query, and targets could be the document.\n",
        "    scoring_examples.append({'inputs': query, 'targets': doc}) # Ensure keys are 'inputs' and 'targets'\n",
        "\n",
        "# Print a sample of scoring_examples to inspect the structure\n",
        "print(\"Sample scoring_examples structure:\", scoring_examples[:2])\n",
        "\n",
        "\n",
        "# Define output features for scoring.\n",
        "# This might need adjustment based on your model's scoring output.\n",
        "output_features_ir = {\n",
        "        \"inputs\":\n",
        "            seqio.Feature(\n",
        "                vocabulary=model_ir.input_vocabulary, add_eos=True),\n",
        "        \"targets\": # Targets here represent the documents being scored against the query\n",
        "            seqio.Feature(\n",
        "                vocabulary=model_ir.output_vocabulary, add_eos=True)\n",
        "    }\n",
        "features_ir = dict(sorted(output_features_ir.items()))\n",
        "\n",
        "# Define preprocessors for your document and query data.\n",
        "# These should align with how your chosen IR model was trained.\n",
        "preprocessors_ir = [\n",
        "    seqio.preprocessors.tokenize,\n",
        "    seqio.preprocessors.append_eos\n",
        "    # Add other relevant preprocessors (e.g., truncation, padding)\n",
        "]\n",
        "\n",
        "# Convert examples to dataset\n",
        "# You might need to adapt this part significantly for your data source\n",
        "dataset_ir = get_dataset_from_natural_text_examples(\n",
        "    scoring_examples, # Use your scoring examples\n",
        "    preprocessors=preprocessors_ir,\n",
        "    task_feature_lengths=document_feature_lengths,\n",
        "    features=features_ir)\n",
        "\n",
        "feature_converter_ir = model_ir.FEATURE_CONVERTER_CLS(pack=False)\n",
        "model_dataset_ir = feature_converter_ir(\n",
        "    dataset_ir, task_feature_lengths=document_feature_lengths)\n",
        "\n",
        "infer_dataset_ir = tf.data.Dataset.zip((dataset_ir, model_dataset_ir))\n",
        "\n",
        "infer_dataset_ir = infer_dataset_ir.padded_batch(\n",
        "    batch_size_ir, drop_remainder=False).enumerate()\n",
        "infer_dataset_iter_ir: Iterator[Tuple[int, Any]] = iter(\n",
        "    infer_dataset_ir.prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1bb5ecf"
      },
      "source": [
        "### Define and Run Scoring Function\n",
        "\n",
        "This cell sets the inference mode to scoring and runs the scoring function on your document data and query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa147856"
      },
      "source": [
        "# Set inference mode to SCORE\n",
        "mode_ir = InferenceType.SCORE\n",
        "\n",
        "if mode_ir == InferenceType.PREDICT_WITH_AUX:\n",
        "  infer_step_ir = model_ir.predict_batch_with_aux\n",
        "elif mode_ir == InferenceType.SCORE:\n",
        "  infer_step_ir = model_ir.score_batch\n",
        "else:\n",
        "  raise ValueError(\"Mode must be `predict_with_aux`, or `score`,\"\n",
        "                  f\" but instead was {mode_ir}.\")\n",
        "\n",
        "infer_fn_ir = functools.partial(\n",
        "  utils.get_infer_fn(\n",
        "    infer_step=infer_step_ir,\n",
        "    batch_size=batch_size_ir,\n",
        "    train_state_axes=train_state_initializer_ir.train_state_axes,\n",
        "    partitioner=partitioner_ir),\n",
        "  train_state=train_state_ir)\n",
        "\n",
        "# Run scoring\n",
        "all_scores = []\n",
        "for chunk, chunk_batch in infer_dataset_iter_ir:\n",
        "    chunk_dataset = tf.data.Dataset.from_tensor_slices(chunk_batch)\n",
        "    chunk_dataset.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    task_dataset = chunk_dataset.map(\n",
        "        lambda p, m: p, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    model_dataset = chunk_dataset.map(\n",
        "        lambda p, m: m, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    chunk_rng = jax.random.fold_in(jax.random.PRNGKey(0), chunk)\n",
        "\n",
        "    # In scoring mode, _extract_tokens_and_aux_values will return None for predictions\n",
        "    # and the scores in aux_values.\n",
        "    inferences_ir = _extract_tokens_and_aux_values(\n",
        "        infer_fn_ir(model_dataset.enumerate(), rng=chunk_rng))\n",
        "\n",
        "    predictions_ir, aux_values_ir = inferences_ir\n",
        "\n",
        "    # Extract scores from aux_values\n",
        "    if 'scores' not in aux_values_ir:\n",
        "        raise ValueError(\"Expected 'scores' in aux_values when mode is SCORE.\")\n",
        "\n",
        "    scores_batch = aux_values_ir['scores']\n",
        "    all_scores.extend(scores_batch)\n",
        "\n",
        "# Associate scores with documents\n",
        "scored_documents = []\n",
        "for i, score in enumerate(all_scores):\n",
        "    # Assuming a one-to-one correspondence between scoring_examples and scores\n",
        "    scored_documents.append({'document': scoring_examples[i]['targets'], 'score': score})\n",
        "\n",
        "# Sort documents by score in descending order\n",
        "scored_documents.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "print(\"Documents ranked by relevance to the query:\")\n",
        "for item in scored_documents:\n",
        "    print(f\"Document: {item['document']}, Score: {item['score']}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "074bf90b"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "You've now seen the core modifications needed to adapt the notebook for information retrieval using scoring.\n",
        "\n",
        "To fully implement this, you will need to:\n",
        "\n",
        "1.  **Choose and configure a T5X model** suitable for your information retrieval task. This might involve using a pre-trained model or fine-tuning on a relevant dataset. Update the model definition in the \"Modified Model and Data Configuration\" cell.\n",
        "2.  **Implement your data loading and preprocessing logic** in the \"Prepare Document Data for Scoring\" cell. This is crucial for handling your specific document data and formatting it correctly for your chosen T5X model.\n",
        "3.  **Refine the scoring and ranking logic** in the \"Define and Run Scoring Function\" cell based on how your model outputs scores and how you want to rank the results.\n",
        "\n",
        "Let me know if you'd like to explore any of these steps in more detail!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to T5X: Inference Deep Dive",
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}